#cloud-config

users:
  - name: ubuntu
    shell: /usr/bin/zsh
    sudo: ['ALL=(ALL) NOPASSWD:ALL']
    groups: sudo
    passwd: password
    lock_passwd: false

package_upgrade: true

ssh_pwauth: true
chpasswd:
  list: |
     ubuntu:password
  expire: False

packages:
  - zsh
  - openjdk-11-jdk
  - git
  - curl
  - wget

runcmd:
  # Set up environment variables
  - |
    export JAVA_HOME=$(dirname $(dirname $(readlink -f /usr/bin/java)))
    echo "export JAVA_HOME=${JAVA_HOME}" >> /etc/profile.d/java_home.sh
    java -version || exit 1

  # Scala Setup
  - wget https://downloads.lightbend.com/scala/2.13.8/scala-2.13.8.tgz -O /tmp/scala-2.13.8.tgz
  - tar -xvzf /tmp/scala-2.13.8.tgz -C /usr/local
  - echo 'export PATH=$PATH:/usr/local/scala-2.13.8/bin' | tee -a /etc/profile.d/scala_env.sh

  # sbt setup
  - echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | sudo tee /etc/apt/sources.list.d/sbt.list
  - echo "deb https://repo.scala-sbt.org/scalasbt/debian /" | sudo tee /etc/apt/sources.list.d/sbt_old.list
  - curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | sudo apt-key add
  - sudo apt-get update
  - sudo DEBIAN_FRONTEND=noninteractive apt-get install -y sbt

  # Download and Extract Hadoop
  - mkdir -p /opt/hadoop
  - curl -o /opt/hadoop/hadoop-3.3.6.tar.gz -OL https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
  - tar -xzf /opt/hadoop/hadoop-3.3.6.tar.gz -C /opt/hadoop
  - mv /opt/hadoop/hadoop-3.3.6 /opt/hadoop/hadoop

  # Set HADOOP_HOME and update environment variables
  - echo 'export HADOOP_HOME=/opt/hadoop/hadoop' | tee -a /etc/profile.d/hadoop_env.sh
  - echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin' | tee -a /etc/profile.d/hadoop_env.sh
  - echo 'export HADOOP_MAPRED_HOME=$HADOOP_HOME' | tee -a /etc/profile.d/hadoop_env.sh
  - echo 'export HADOOP_COMMON_HOME=$HADOOP_HOME' | tee -a /etc/profile.d/hadoop_env.sh
  - echo 'export HADOOP_HDFS_HOME=$HADOOP_HOME' | tee -a /etc/profile.d/hadoop_env.sh
  - echo 'export YARN_HOME=$HADOOP_HOME' | tee -a /etc/profile.d/hadoop_env.sh
  - echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native' | tee -a /etc/profile.d/hadoop_env.sh

  # Update JAVA_HOME in Hadoop's hadoop-env.sh
  - |
    JAVA_HOME_PATH=$(readlink -f /usr/bin/java | sed "s:bin/java::")
    HADOOP_ENV_SH="/opt/hadoop/hadoop/etc/hadoop/hadoop-env.sh"
    grep -q 'export JAVA_HOME=' $HADOOP_ENV_SH && sed -i "/export JAVA_HOME=/c\export JAVA_HOME=${JAVA_HOME_PATH}" $HADOOP_ENV_SH || echo "export JAVA_HOME=${JAVA_HOME_PATH}" >> $HADOOP_ENV_SH
  
  # Validate Hadoop installation
  - source /etc/profile && hadoop version

  # Backup original Hadoop configuration files
  - cp /opt/hadoop/hadoop/etc/hadoop/hadoop-env.sh /opt/hadoop/hadoop/etc/hadoop/hadoop-env.sh.backup
  - cp /opt/hadoop/hadoop/etc/hadoop/core-site.xml /opt/hadoop/hadoop/etc/hadoop/core-site.xml.backup
  - cp /opt/hadoop/hadoop/etc/hadoop/hdfs-site.xml /opt/hadoop/hadoop/etc/hadoop/hdfs-site.xml.backup
  - cp /opt/hadoop/hadoop/etc/hadoop/mapred-site.xml /opt/hadoop/hadoop/etc/hadoop/mapred-site.xml.backup
  - cp /opt/hadoop/hadoop/etc/hadoop/yarn-site.xml /opt/hadoop/hadoop/etc/hadoop/yarn-site.xml.backup

 
  # Configure core-site.xml
  - |
    echo '<configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
      </property>
    </configuration>' > /opt/hadoop/hadoop/etc/hadoop/core-site.xml

  # Configure hdfs-site.xml
  - |
    echo '<configuration>
      <property>
        <name>dfs.replication</name>
        <value>1</value>
      </property>
      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///opt/hadoop_tmp/hdfs/namenode</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///opt/hadoop_tmp/hdfs/datanode</value>
      </property>
    </configuration>' > /opt/hadoop/hadoop/etc/hadoop/hdfs-site.xml

  # Configure mapred-site.xml
  - |
    echo '<configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
    </configuration>' > /opt/hadoop/hadoop/etc/hadoop/mapred-site.xml

  # Configure yarn-site.xml
  - |
    echo '<configuration>
      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>
      <property>
        <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
      </property>
    </configuration>' > /opt/hadoop/hadoop/etc/hadoop/yarn-site.xml

  # Create Hadoop data directories
  - mkdir -p /opt/hadoop_tmp/hdfs/namenode
  - mkdir -p /opt/hadoop_tmp/hdfs/datanode
  - chown -R ubuntu:ubuntu /opt/hadoop_tmp

    # SSH setup for Hadoop
  - sudo -u ubuntu ssh-keygen -t rsa -P '' -f ~ubuntu/.ssh/id_rsa
  - cat ~ubuntu/.ssh/id_rsa.pub >> ~ubuntu/.ssh/authorized_keys
  - chown ubuntu:ubuntu ~ubuntu/.ssh/id_rsa*
  - chown ubuntu:ubuntu ~ubuntu/.ssh/authorized_keys

  # Initialize HDFS and start Hadoop services
  - sudo -u ubuntu /opt/hadoop/hadoop/bin/hdfs namenode -format
  - sudo -u ubuntu /opt/hadoop/hadoop/sbin/start-dfs.sh
  - sudo -u ubuntu /opt/hadoop/hadoop/sbin/start-yarn.sh

  # Validate HDFS setup
  - sudo -u ubuntu /opt/hadoop/hadoop/bin/hdfs dfs -mkdir /test
  - sudo -u ubuntu /opt/hadoop/hadoop/bin/hdfs dfs -ls /
  
  - echo "source /etc/profile.d/hadoop_env.sh" >> ~ubuntu/.zshrc
  - echo "source /etc/profile.d/hadoop_env.sh" >> ~ubuntu/.bashrc


final_message: "Hadoop is up, after $UPTIME seconds"
